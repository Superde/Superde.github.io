<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.0.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"dymzcc.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="三环三星">
<meta property="og:url" content="https://dymzcc.github.io/page/2/index.html">
<meta property="og:site_name" content="三环三星">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="dede">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://dymzcc.github.io/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>三环三星</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">三环三星</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">学习，记录，分享</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dymzcc.github.io/2020/09/11/Community%20Detection%20Using%20Restrained%20Random-walk%20Similarity/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="dede">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="三环三星">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/09/11/Community%20Detection%20Using%20Restrained%20Random-walk%20Similarity/" class="post-title-link" itemprop="url">Community Detection Using Restrained Random-walk Similarity</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-09-11 21:22:33" itemprop="dateCreated datePublished" datetime="2020-09-11T21:22:33+08:00">2020-09-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-10-06 21:00:13" itemprop="dateModified" datetime="2020-10-06T21:00:13+08:00">2020-10-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Community-Detection-Using-Restrained-Random-walk-Similarity"><a href="#Community-Detection-Using-Restrained-Random-walk-Similarity" class="headerlink" title="Community Detection Using Restrained Random-walk Similarity"></a>Community Detection Using Restrained Random-walk Similarity</h1><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>&emsp;&emsp;从一个节点开始长度有限的随机游走，如果这个节点经过了与之相似的节点集合，则认为这个节点也属于这个节点集合（即社区）。这个方法的提出是基于下列的思想：从初始节点出发进行游走，游走到和这个节点同属于一个社区的节点集合，那么初始节点和这个节点集合一定是相似的，因为初始节点最初更倾向于在该社区内进行游走。</p>
<p>在以下两条件下，此方法效果更强：</p>
<p><strong>1. 多次游走，删除不常访问节点</strong></p>
<p>&emsp;&emsp;首先是避免异常的随机游走，因为游走有较小概率离开初始节点所在的社区，导致错误的聚类。异常随机游走的起始顶点被错误地聚类。<strong>因此，从每个节点出发，进行多次随机游走，将很少游走到的节点从可能游走经过的节点集合中删除。多次实验，就能减少小概率事件发生带来的偏差。</strong></p>
<p><strong>2. 适时停止随机游走</strong></p>
<p>&emsp;&emsp;在适当时间强制终止随机游走，根据观察：在第1阶段，随机游走会经常访问没有访问过的节点；在第2阶段，随机游走会重复访问已经访问过得节点，此时仍在初始节点所在的社区中游走；在第3阶段，随机游走会从最初的社区游走到其他社区。在理想情况下，应该在第二阶段停止随机游走。</p>
<p>&emsp;&emsp;第二个条件对于规模大，规模小的社区发现都很有效。为了发现规模较大的社区，随机游走的长度需要更大；为了发现规模较小的社区，随机游走的长度需要短一些。总之，需要确定合适的随机游走长度阻止游走离开最初的社区。即定义一个阈值，自适应决定游走的步长。</p>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p>（1）Traditional Graph Partitioning（2）Modularity Optimization（3）Potts Mode（4）Random Walk（5）Statistical inference （6）Other Techniques</p>
<h2 id="3-Restrained-Random-Walk-Similarity-Method"><a href="#3-Restrained-Random-Walk-Similarity-Method" class="headerlink" title="3. Restrained Random-Walk Similarity Method"></a>3. Restrained Random-Walk Similarity Method</h2><h3 id="3-1-Random-Walk-Similarity-Method"><a href="#3-1-Random-Walk-Similarity-Method" class="headerlink" title="3.1 Random-Walk Similarity Method"></a>3.1 Random-Walk Similarity Method</h3><p>&emsp;&emsp;Random-Walk Similarity Method是Restrained Random-Walk Similarity Method的基础，用下图简单介绍一下Random-Walk Similarity Method：</p>
<p><img src="https://dede-photo.oss-cn-beijing.aliyuncs.com/img/随机游走相似性.png" alt="随机游走相似性" style="zoom:50%;" /></p>
<p>&emsp;&emsp;从社区1中的节点开始随机游走，一段时间内，游走更倾向在社区1内游走，因为从社区1到社区2的边很少，游走出去的概率就小。同样地，从社区2开始的随机游走也是更倾向于在社区2内游走。基于随机游走的这种特性，如果在从所有节点执行有限长度的随机游走之后，游走经过的节点集之间存在高度相似性，因此本文的方法会将随机游走的起始节点和这些经过的节点聚集到相同的社区中。</p>
<p><img src="https://dede-photo.oss-cn-beijing.aliyuncs.com/img/Random Walk Similarity Method.png" alt="Random Walk Similarity Method" style="zoom:50%;" /></p>
<p>&emsp;&emsp;详细步骤如Algorithm 1所示：在3-12行，每个节点$（v_1,…v_N）$都进行长度为$m$的随机游走，并获得随机游走序列$S_1,…S_N$。理想情况下，$S_i$应该只包括初始节点$v_i$和同个社区内的节点。然而，随机游走可能会从初始节点所在社区游走到其他社区，甚至可能在刚开始游走不久就离开了初始社区，因此集合$S_i$会包括了$v_i$以及和$v_i$不属于同个社区的节点。为了避免这种情况，在4-7行中，每个节点重复$p$次随机游走，并且只有频繁访问的节点才会被添加到$S_i$中（第8-11行）。在13-15行，节点$v_1,…v_N$基于集合$S_1,…,S_N$之间的Jaccard相似系数进行聚类。如果$sim(S_i,S_j)\geq th_{sim}$，节点$v_i$和$v_j$则归为同一类。还有一种特殊情况，也把$v_i$和$v_j$归为一类，没看懂，不谈。</p>
<h3 id="3-2-Restraning-a-Random-Walk"><a href="#3-2-Restraning-a-Random-Walk" class="headerlink" title="3.2 Restraning a Random Walk"></a>3.2 Restraning a Random Walk</h3><p>&emsp;&emsp;在Random Walk Similarity中，很难去决定随机游走的长度（m），尤其是当图中有规模大的社区和规模小的社区的时候。然而，当m偏小时，尽管$v_i$和$v_j$处于同一个大规模社区中，$sim(S_i,S_j)$会偏小，因此大规模社区会被识别成多个小规模社区；而当m偏大时，从小社区中初始节点开始的随机游走会游走到其他社区中，因此$sim(S_i,S_j)$会偏高，小规模社区中的初始节点会被错误归类到其他社区中，导致无法识别小规模社区。</p>
<p>&emsp;&emsp;为了解决这个问题，当随机游走已经经过了很多初始社区中的节点，且随机游走还没游走到另一个社区时，重新进行随机游走。</p>
<p>&emsp;&emsp;首先考虑随机游走的步长和游走经过的唯一节点数之间的关系。下图表示Japanese temple/shrine数据集中具有17个节点的社区，从该社区中某点开始随机游走的过程：</p>
<p><img src="https://dede-photo.oss-cn-beijing.aliyuncs.com/img/随机游走.png" alt="随机游走" style="zoom:50%;" /></p>
<p>&emsp;&emsp;第一阶段，随机游走会大概率游走到未访问过的节点；第二阶段，随机游走会经常游走到已经访问过得节点；第三阶段，随机游走到新的社区，会访问未访问过的节点。</p>
<p>&emsp;&emsp;基于上述观察，随机游走应该在第二阶段终止。当给定步长$w$下，随机游走访问的unique节点的增长$(n_i-n_{i-(w-1)})$小于算法中给定阈值，即停止随机游走，具体算法如下所示：</p>
<p><img src="https://dede-photo.oss-cn-beijing.aliyuncs.com/img/Restrained random walk.png" alt="Restrained random walk" style="zoom:50%;" /></p>
<p>&emsp;&emsp;思路和Random-Walk Similarity Method差不多，只是加了游走停止的规则。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dymzcc.github.io/2020/09/05/Adaptive%20Graph%20Encoder%20for%20Attributed%20Graph%20Embedding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="dede">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="三环三星">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/09/05/Adaptive%20Graph%20Encoder%20for%20Attributed%20Graph%20Embedding/" class="post-title-link" itemprop="url">Adaptive Graph Encoder for Attributed Graph Embedding</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-09-05 11:04:33" itemprop="dateCreated datePublished" datetime="2020-09-05T11:04:33+08:00">2020-09-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-10-06 21:00:07" itemprop="dateModified" datetime="2020-10-06T21:00:07+08:00">2020-10-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Adaptive-Graph-Encoder-for-Attributed-Graph-Embedding"><a href="#Adaptive-Graph-Encoder-for-Attributed-Graph-Embedding" class="headerlink" title="Adaptive Graph Encoder for Attributed Graph Embedding"></a>Adaptive Graph Encoder for Attributed Graph Embedding</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>​        属性图嵌入是图分析中具有挑战性的一项任务。目前GCN已经取得了引人注目的成果，但是GCN存在着以下缺点：（1）实验表明，图卷积滤波器（即图卷积核）和权重矩阵的纠缠会损害性能和鲁棒性（2）我们证明了这些方法中的图卷积滤波器是广义拉普拉斯平滑滤波器的特例，但他们并没有保留最佳的低通特征（3）现有算法的训练目标通常是恢复与现实应用并不总是一致的邻接矩阵或特征矩阵。</p>
<p>​        为了解决这些问题，我们提出了AGE（Adaptive Graph Encoder），AGE由两部分组成：（1）为了更好地减轻节点特征中的高频噪声，AGE应用了特殊的拉普拉斯平滑滤波器（2）AGE采用了自编码器，可以迭代地增强滤波后的特征，以实现更好的节点嵌入。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>​        属性图应用广泛，为了对属性图进行分析，研究人员已经提出了多种基于机器学习的方法，来进行诸如节点分类，链接预测等属性图分析任务。但是由于高维，非欧式的图结构特征和丰富的节点属性特征，属性图嵌入仍面临着巨大挑战。</p>
<p>​        网络表示学习旨在将高维的图结构编码为低维的实值的向量形式，早期的图嵌入方法基于拉普拉斯特征图，矩阵分解，随机游走。但是这些方法属于浅结构，受到诸多限制。</p>
<p>​        最近，图上的深度学习非常流行，并且GCN及其变体在图学习任务上取得了非常好的表现，如属性图嵌入任务。在这些方法大多基于GAE（Graph Autoencoder）和VGAE（Variational Graph Autoencoder），如下图所示。但是这些基于GCN的方法有三个主要缺点：</p>
<p><img src="https://dede-photo.oss-cn-beijing.aliyuncs.com/img/图1：图自编码器.png" alt="图1：图自编码器" style="zoom:50%;" /></p>
<ul>
<li><p>GCN编码器有多个图卷积层，每层包含了一个图卷积核（图1中的H），一个权重矩阵（图1中的$W_1$和$W_2$）以及一个激活函数。然而，先前的工作表明，卷积核和权重矩阵的纠缠无法为半监督图表示学习提供性能增益，甚至会损害训练效率，因为它加深了反向传播的路径。本文中，我们通过受控实验将这一结论进一步拓展到无监督的场景中，表明本文的解纠缠体系结构比纠缠模型具有更好，更强大的功能。</p>
</li>
<li><p>先前的工作证明了图卷积滤波器即为在特征矩阵上应用拉普拉斯平滑滤波器进行低通噪声处理。但是我们证明，现有的图卷积滤波器不是最佳的低通滤波器，因为它们不能滤除某些高频间隔中的噪声，因此，它们无法到达最佳平滑效果。</p>
</li>
<li><p>我们认为，这些算法的训练目标（重建邻接矩阵或特征矩阵）与现实的应用不兼容。重建邻接矩阵，即尽力与真实链接信息拟合，然而在缺乏特征信息的情况下，并不合适；重建特征矩阵，会强使模型记住特征中的高频特征，因此也不适合。</p>
<p>​    基于以上原因，我们提出了AGE框架。</p>
</li>
</ul>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><h3 id="2-1-Conventional-Graph-Embedding"><a href="#2-1-Conventional-Graph-Embedding" class="headerlink" title="2.1 Conventional Graph Embedding"></a>2.1 Conventional Graph Embedding</h3><p>​        常规图嵌入：（1）基于拉普拉斯特征矩阵（2）基于矩阵分解（3）基于随机游走</p>
<h3 id="2-2-GCN-based-Graph-Embedding"><a href="#2-2-GCN-based-Graph-Embedding" class="headerlink" title="2.2 GCN-based Graph Embedding"></a>2.2 GCN-based Graph Embedding</h3><p>​        由于GCN的强大表示学习能力，基于GCN的属性图嵌入方法目前是性能最佳的。对于缺少标签信息的无监督图嵌入，可以将基于GCN的优化目标分为两类：</p>
<p><strong>1. Reconstruct the adjacency matrix</strong></p>
<p>​        这类方法主要通过学习嵌入向量来恢复局部邻域结构。GAE和VGAE将GCn作为编码器来进行嵌入学习，然后通过交叉熵损失对内积进行解码。</p>
<p><strong>2. Reconstruct the feature matrix</strong></p>
<p>​        这种模型是节点特征矩阵的自编码器，而邻接矩阵仅充当过滤器。</p>
<h2 id="3-Proposed-Method"><a href="#3-Proposed-Method" class="headerlink" title="3. Proposed Method"></a>3. Proposed Method</h2><h3 id="3-1-Problem-Formalization"><a href="#3-1-Problem-Formalization" class="headerlink" title="3.1 Problem Formalization"></a>3.1 Problem Formalization</h3><p>​        给定一属性图${\cal G=(V,E},X)$，其中${\cal V}=\{v_1,v_2,…,v_n\}$是点集，$\cal E$是边集，$X=[x_1,x_2,…,x_n]^T$是特征矩阵。图$\cal G$的拓扑结构可以用邻接矩阵$A={a_{ij}}\in{\Bbb R}^{n\times n}$表示，其中如果$v_i$和$v_j$相连，则$a_{ij}=1$。$D=diag(d_1,d_2,…,d_n)\in {\Bbb R}^n\times n$表示邻接矩阵$A$的度矩阵，$d_i=\sum_{v_i\in{\cal V}}a_{ij}$为节点$v_i$的度矩阵。拉普拉斯矩阵定义为$L=D-A$。</p>
<p>​        属性图嵌入的目的即将节点映射为低维向量，我们定义$Z$为嵌入矩阵，嵌入向量应该保留图$\cal G$中的拓扑结构信息和属性特征信息。</p>
<p>​        我们将节点聚类和链接预测作为下游任务。节点聚类任务旨在节点划分为不相交的组${G_1,G_2,…,G_m}$，其中相似的节点属于同一组。链接预测任务即预测两个节点之间是否存在边。</p>
<h3 id="3-2-Overall-Framework"><a href="#3-2-Overall-Framework" class="headerlink" title="3.2 Overall Framework"></a>3.2 Overall Framework</h3><p>​        本文模型由两部分组成：</p>
<ul>
<li><strong>Laplacian Smoothing Filter：</strong>设计的滤波器$H$用作低通滤波器，以对特征矩阵$X$的高频分量进行消噪。平滑后的特征矩阵$\tilde X$被用作自适应编码器的输入。</li>
<li><strong>Adaptive Encoder:</strong>为了获得更有表达能力的节点嵌入，该模块通过自适应选择高度相似或不相似的节点对来构建训练集，然后以监督的方式训练编码器。</li>
</ul>
<p><img src="https://dede-photo.oss-cn-beijing.aliyuncs.com/img/AGE框架.png" alt="AGE框架" style="zoom: 50%;" /></p>
<h3 id="3-3-Laplacian-Smoothing-Filter"><a href="#3-3-Laplacian-Smoothing-Filter" class="headerlink" title="3.3 Laplacian Smoothing Filter"></a>3.3 Laplacian Smoothing Filter</h3><p>​        图学习的基本假设是：图上附近节点应该是相似的，因此节点特征在图流形上应该是平滑的。</p>
<p><strong>1. Analysis of Smooth Signals：</strong></p>
<p>​        我们首先从图形信号处理的角度解释平滑，把$x\in {\Bbb R}^T$作为图形信号，其中为每个节点分配了一个标量，定义过滤器矩阵为$H$。为了测量图信号$x$的平滑性，我们可以计算Rayleigh Quotient：</p>
<script type="math/tex; mode=display">
R(L,x)=\frac{x^{\sf T}Lx}{x^{\sf T}x}=\frac{\sum_{(i,j)\in{\cal E}}(x_i-x_j)^2}{\sum_{i\in {\cal V}}x_i^2}
\tag{1}</script><p>​        这个商实际上是$x$的归一化方差得分。如上所述，平滑信号应该在相邻节点上分配相似的值，因此，假定具有较低Rayleigh Quotient的信号更平滑。</p>
<p>​        拉普拉斯算子的特征分解$L=U\Lambda U^{-1}$，其中$U\in {\Bbb R}^{n\times n}$包含了特征向量，$\Lambda=diag(\lambda_1,\lambda_2,….,\lambda_n)$是特征值的对角矩阵，本证向量$u_i$的平滑度如下定义：</p>
<script type="math/tex; mode=display">
R(L,u_i)=\frac{u_i^{\sf T}Lu_i}{u_i^{\sf T}u_i}=\lambda_i
\tag{2}</script><p>​        公式（2）表示更平滑的特征向量与更小的特征值相关，更小的特征值意味着评率更低。因此，我们基于基于公式（1）（2）分解信号$x$：</p>
<script type="math/tex; mode=display">
x=Up=\sum_{i=1}^np_iu_i
\tag{3}</script><p>式中：$p_i$是$u_i$的特征向量系数，信号$x$的平滑度实际为：</p>
<script type="math/tex; mode=display">
R(L,x)=\frac{x^{\sf T}Lx}{x^{\sf T}x}=\frac{\sum_{i=1}^np_i^2\lambda_i}{\sum_{i=1}^np_i^2}
\tag{4}</script><p>因此，为了获得更平滑的信号，<strong>设计滤波器的目标为：滤除高频分量的同时保留低频分量</strong>。由于其高计算效率和令人信服的性能，拉普拉斯平滑滤波器通常用于此目的。</p>
<p><strong>2. Generalized Laplacian Smoothing Filter：</strong></p>
<p>​        广义拉普拉斯平滑滤波器定义如下：</p>
<script type="math/tex; mode=display">
H=I-kL
\tag{5}</script><p>式中：$k$为实值。用$H$作为滤波器矩阵，滤波信号$x$如下所示：</p>
<script type="math/tex; mode=display">
\tilde x=Hx=U(I-k\Lambda)U^{-1}Up=\sum_{i=1}^n(1-k\lambda_i)p_iu_i=\sum_{i=1}^np_i'u_i
\tag{6}</script><p>因此，为了实现低通滤波，频率响应函数$1-k\lambda$应该是一个减量和非负函数。堆叠$t$个拉普拉斯平滑滤波器，我们将滤波后的特征矩阵$X$表示为：</p>
<script type="math/tex; mode=display">
\tilde X=H^tX
\tag{7}</script><p>注意，过滤器完全是非参数的。</p>
<p><strong>3. The Choice of $k$.：</strong></p>
<p>​        $\tilde A=I=A$，我们使用对称归一化图拉普拉斯算子：</p>
<script type="math/tex; mode=display">
\tilde L_{sym}=\tilde D^{-\frac{1}{2}}\tilde L\tilde D^{-\frac{1}{2}}
\tag{8}</script><p>式中：$\tilde D$和$\tilde L$是$\tilde A$的度矩阵和拉普拉斯矩阵。然后滤波器可以如下定义：</p>
<script type="math/tex; mode=display">
H=I-k\tilde L_{sym}
\tag{9}</script><p>注意：如果我们设置$k=1$，滤波器就成了GCN滤波器。</p>
<p>​        为了选择最佳的$k$，特征值$\tilde \Lambda$的分布（分解$\tilde L_{sym}=\tilde U\tilde \Lambda\tilde U^{-1}$得到)应该被仔细分析，$\tilde x$的平滑度为：</p>
<script type="math/tex; mode=display">
R(L,\tilde x)=\frac{\tilde x^{\sf T}L\tilde x}{\tilde x^{\sf T}\tilde x}=\frac{\sum_{i=1}^np_i'^2\lambda_i}{\sum_{i=1}^np_i'^2}
\tag{10}</script><p>因此，$p_i’^2$应该随着$\lambda_i$的增加而减少。定义最大的特征值为$\lambda_{max}$，理论上，如果$k&gt;\frac{1}{\lambda_{max}}$，滤波器在$(1/k,\lambda_{max}]$区间内无法通过，因为$p_i’^2$在此区间内增大。此外，如果$k&lt;\frac{1}{\lambda_{max}}$，滤波器无法过滤所有高频成分。所以，$k=\frac{1}{\lambda_{max}}$是最佳选择。</p>
<p>​        已经有研究人员证明：拉普拉斯特征值的范围在0到2之间，因此，GCN滤波器在$(1,2]$区间内不是低通的。我们的实验证明：在重新规范化后，最大的特征值$\lambda_{max}$会缩小到3/2左右，这时，1/2已不是最佳选择。在本文的实验中，我们计算每个数据集的$\lambda_{max}$，然后设置$k=\frac{1}{\lambda_{max}}$。我们也进一步分析了不同的$k$的影响。</p>
<h3 id="3-4-Adaptive-Encoder"><a href="#3-4-Adaptive-Encoder" class="headerlink" title="3.4 Adaptive Encoder"></a>3.4 Adaptive Encoder</h3><p>​        经过$t$层的拉普拉斯平滑的过滤后，输出的特征更加平滑，同时也保留了丰富的属性信息。</p>
<p>​        为了学习更好的节点嵌入,我们需要找到合适的无监督优化目标。最后，受到深度自适应学习（Deep Adaptive Learning）的启发，我们利用了成对节点相似性。对于属性图嵌入任务，两个节点之间的关系至关重要，这要求训练目标必须是合适的相似度度量。基于GAE的方法通常选择邻接矩阵作为节点对的真实标签。但是我们认为邻接矩阵只能记录1-hop的结构信息，这是不够的。同时，我们认为，由于平滑特征或经过训练的嵌入的相似性更加准确，因为它们将结构和特征融合在了一起。最后，我们自适应地选择相似度高的节点对作为正样本，而相似度低的节点对作为负样本。</p>
<p>​        给定过滤后的特征矩阵$\tilde X$，节点嵌入由线性编码器$f$编码：</p>
<script type="math/tex; mode=display">
Z=f(\tilde X;W)=\tilde XW
\tag{11}</script><p>式中：$W$是权重矩阵。然后使用min-max所放弃将嵌入缩放到$[0,1]$区间，以减少方差。为了测量节点的成对相似性，我们利用余弦函数来实现我们的相似性度量。相似度矩阵$S$定义如下：</p>
<script type="math/tex; mode=display">
S=\frac{ZZ^{\sf T}}{||Z||_2^2}
\tag{12}</script><p><strong>1. Training Sample Selection：</strong></p>
<p>​        在计算相似度矩阵后，我们将成对相似性序列进行降序排列。$r_{ij}$为节点对$(v_i,v_j)$的排序。定义正样本的最高排序为$r_{pos}$，负样本的最低排序为$r_{neg}$，因此生成节点对$(v_i,v_j)$标签为：</p>
<script type="math/tex; mode=display">
l_{ij}=\left\{
\begin{aligned}
&1  \qquad \qquad r_{ij}\leq r_{pos}    \\
&0  \qquad \qquad r_{ij}> r_{pos}    \\
&None \qquad  otherwise
\end{aligned}
\right.
\tag{13}</script><p>​        通过这种方法，$r_{pos}$个正样本，$n^2-r_{neg}$个负样本的训练集就构建成了。特别地，由于编码器没有经过训练，在第一次构造训练集的时候，我们直接将平滑后的特征用于初始化$S$：</p>
<script type="math/tex; mode=display">
S=\frac{\tilde X\tilde X^{\sf T}}{||\tilde X||_2^2}
\tag{14}</script><p>​        构件训练集后，我们可以在有监督的方式下训练编码器。在现实世界的图中，不相似的节点对总是比相似的节点对多得多，因此我们在训练集中选择的负样本数要多于$r_{pos}$。为了平衡正样本和负样本，我们在每个epoch中随机选择$r_{pos}$个负样本。平衡后的样本集合定义为$O$。最后，交叉熵损失函数定义如下：</p>
<script type="math/tex; mode=display">
{\cal L}=\sum_{(v_i,v_j)\in O}-l_{ij}log(s_{ij})-(1-l_{ij})log(1-s_{ij})
\tag{15}</script><p><strong>2. Thresholds Update：</strong></p>
<p>​        受到curriculum learning的启发，我们设计了一种特别的$r_{pos}$和$r_{neg}$更新策略，以控制训练集的规模。在训练过程开始时，会为编码器选择更多样本以找到粗糙的聚类模式。之后，仍保留较高置信度的样本用于训练，迫使编码器捕获精炼的模式。在实践中，随着训练过程的进行，$r_{pos}$减少，$r_{neg}$增加。我们设置初始阈值为$r_{pos}^{st},r_{neg}^{st}$，设置最终阈值为$r_{pos}^{ed},r_{neg}^{ed}$，使$r_{pos}^{ed}\leq r_{pos}^{st},r_{neg}^{ed}\geq r_{neg}^{st}$。假设阈值更新$T$次，更新策略可表示为：</p>
<script type="math/tex; mode=display">
r'_{pos}=r_{pos}+\frac{r_{pos}^{ed}-r_{pos}^{st}}{T}
\tag{16}</script><script type="math/tex; mode=display">
r'_{neg}=r_{neg}+\frac{r_{neg}^{ed}-r_{neg}^{st}}{T}
\tag{17}</script><p>​        随着训练的进行，每次更新阈值时，我们都会重建训练集并保存嵌入。对于节点聚类任务，我们对保存的嵌入的相似矩阵进行Spetral Clustering，并在根据DBI选择最佳的epoch；对于链接预测任务，我们在验证集上选择效果最佳的epoch。</p>
<p>​        算法流程图如下所示：</p>
<p><img src="https://dede-photo.oss-cn-beijing.aliyuncs.com/img/算法流程图.png" alt="算法流程图" style="zoom:50%;" /></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dymzcc.github.io/2020/08/29/Community%20Detection%20in%20Attributed%20Graphs:%20An%20Embedding%20Approach/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="dede">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="三环三星">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/08/29/Community%20Detection%20in%20Attributed%20Graphs:%20An%20Embedding%20Approach/" class="post-title-link" itemprop="url">Community Detection in Attributed Graphs: An Embedding Approach</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-08-29 20:49:50" itemprop="dateCreated datePublished" datetime="2020-08-29T20:49:50+08:00">2020-08-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-10-06 21:00:01" itemprop="dateModified" datetime="2020-10-06T21:00:01+08:00">2020-10-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Community-Detection-in-Attributed-Graphs-An-Embedding-Approach"><a href="#Community-Detection-in-Attributed-Graphs-An-Embedding-Approach" class="headerlink" title="Community Detection in Attributed Graphs: An Embedding Approach"></a>Community Detection in Attributed Graphs: An Embedding Approach</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>​        社区发现是一个被广泛研究的基本问题，其旨在发现紧密连接的节点群。随着现实网络中可用于实体的丰富信息的激增，在属性图中发现具有属性的社区意义重大。然而，目前现有的属性社区发现方法直接使用了原始的网络拓扑结构，导致结果不理想。本文中，我们提出了一种新颖的属性社区发现方法，具体而言，基于对社区中紧密连接的结构的观察，我们开发了一种新颖的社区结构嵌入方法，以通过底层社区隶属度对固有的社区结构进行编码。基于节点属性和社区结构嵌入，我们将属性社区发现公式华为非负矩阵分解优化问题。此外，我们精心设计了迭代更新规则，以确保找到一个收敛的解决方法。在19个属性图数据集上进行了广泛的实验，这些数据集具有重叠的和非重叠的真实社区，这表明我们提出的模型CDE可以准确识别归因社区，并且明显优于7种最新方法。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>​        社区作为功能模块广泛而自然地存在于现实世界的网络中，例如社交网络，协作网络和Web网络。社区发现（图聚类）算法是分析和理解大型网络的基础分析工具。在文献中，提出了许多基于网络拓扑结构来发现社区的方法。除了网络拓扑结构外，网络中的节点通常具有属性信息，这对于理解社区意义很重要，例如：引文网络中的论文都有关键词。这种具有节点属性的网络称为属性图。</p>
<p>​        由于网络结构和节点属性这两种信息的不同，它给在属性图中发现有意义的社区带来了挑战。已经提出了很多中结合了拓扑结构信息和节点属性信息的方法。然而，对于无权重网络而言，所有这些方法都直接利用原始网络拓扑信息，该拓扑通过为每条边分配相同的值来忽略固有的社区结构。由于社区内的节点连接紧密，因此在属性图中存在大量紧密连接的子图。从直觉上讲，形成那些紧密连接的子图中的边比连接不同子图的边更容易构建相应的社区。考虑到图1中的例子，图（a）中，有两对相连的节点对$(v_1,v_3)$和$(v_1,v_2)$，其中$v_1$和$v_3$属于相同的社区，$v_1$和$v_2$则属于不同的社区，原始的拓扑结构不会反映这两对节点对之间的差异，因为这两对都分配了相同的值，1。然而$(v_1,v_3)$在构建社区中明显比$(v_1,v_2)$更重要。因此直接利用原始的拓扑结构会导致对节点对的一视同仁，而不管他们是否在同一个紧密连接的结构里。</p>
<p><img src="https://dede-photo.oss-cn-beijing.aliyuncs.com/img/邻接矩阵和社区结构嵌入矩阵对社区结构编码性能的比较.png" alt="邻接矩阵和社区结构嵌入矩阵对社区结构编码性能的比较" style="zoom:50%;" /></p>
<p>​        节点在一个紧密相连的社区中，他们应该共享同质节点属性。作为最先进的方法之一，SCI将网络拓扑结构和节点属性与统一的目标函数结合在一起。然而，SCI不会分解稀疏的节点属性矩阵，而是将其作为适合社区隶属度的基础，由于节点属性矩阵的冗余和噪声，这会降低SCI的有效性。</p>
<p>​        本文中，我们研究了属性社区发现问题，旨在发现节点间紧密相连且具有同类属性的社区。社区是可以重叠的，即一个节点可以属于多个社区。为了解决直接使用原始网络拓扑结构并充分利用节点属性的问题，我们提出了一种新的介于社区结构嵌入的社区发现模型，该模型结合了社区结构和节点属性。具体而言，我们基于对社区中紧密相连的结构的观察，提出了一种新颖的社区结构嵌入方法，以通过底层社区隶属度对固有社区结构进行编码。然后利用节点属性信息和社区结构嵌入，将属性社区发现公式转化为非负矩阵特征优化问题，并精心设计更新规则，以确保找到收敛的解决方法。</p>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p>​        本文相关工作包括了结构图聚类，属性图聚类，网络嵌入。图聚类（社区发现）算法被广泛研究，本文中，我们重点关注图生成模型，图生成模型算法假设边是根据基于社区隶属度定义的概率生成的，他们设计概率模型来拟合真实的网络结构（即根据概率模型生成的图与真实图尽可能的相似）。结构图聚类的另一项相关工作是使用非负矩阵分解（NMF）技术。这些算法基于可将邻接矩阵分解为社区隶属度矩阵的线性组合的原理。这些方法都是直接利用原始玩过拓扑结构，忽略了固有的社区结构。</p>
<p>​        <strong>属性图聚类：</strong>在文献中，已经有多重属性图聚类的方法，其中具有代表性的一种方法是利用一种判别模型，通过选择权重向量将节点属性合并到网络结构模型中。除了构建生成模型之外，其他一些方法还利用NMF技术来组合网络结构和节点属性，最先进的算法之一是SCI，它使用NMF技术将观察到的网络结构和节点属性相结合。然而，SCI直接分解邻接矩阵，而没有专注于分解节点属性矩阵，这忽略了边在形成社区结构中的各种意义，并导致无法充分利用节点属性。</p>
<p>​        不同于这些结构图聚类和属性图聚类算法，我们开发了一种新颖的社区结构嵌入矩阵来编码固有的社区结构，而不是简答地使用观察到的网络拓扑进行社区检测。此外，我们的工作重点是同时分解社区结构嵌入矩阵和节点嵌入矩阵，以产生更好的性能。</p>
<p>​        <strong>网络嵌入：</strong>本文的工作涉及网络嵌入，该网络学习用于表示节点的潜在向量。DeepWalk是具有代表性的网络嵌入算法之一，它通过将随机游走生成的节点序列看做是NLP中的句子来获得节点的向量表示。此外，也有人将社区结构结合到节点嵌入中。不同于现有的网络嵌入方法，本文的工作通过固有的社区隶属度对固有的社区结构进行编码，以用于社区检测。</p>
<h2 id="3-Preliminaries"><a href="#3-Preliminaries" class="headerlink" title="3. Preliminaries"></a>3. Preliminaries</h2><p><strong>问题陈述：</strong></p>
<p>​        给定无向有权重属性图$G=(V,E,W,T)$，$V=\{v_1,v_2,…,v_n\}$为点集，$E\subseteq V\times V$为边集，$W_{ij}$表示边$(v_i,v_j)$的权重，$T\in \{0,1\}_{n\times s}$是节点的属性矩阵，矩阵第$i$行表示了节点$v_i$的s维二元向量。此外，定义邻接矩阵为$A\in \{0,1\}_{n\times n}$。本文主要讨论无向无权重图，但是本文提出的模型可以拓展到有向有权重图。在本文的无权重图$G$中，$W_{ij}=A_{ij}=\{0,1\}$。</p>
<p>​        给定一张属性图$G$，其中有$K$个社区，社区发现任务即中找出$K$个节点群$\cal G= \{g_1,…g_\rm K\}$使得：（1）相同节点群内节点连接非常紧密，不同节点群的节点间连接非常稀疏（2）相同节点群内节点具有相同的属性，不同节点群内的节点具有不同的属性。另外，本文中的社区发现问题是允许社区间存在重叠，即$g_i\cap g_j \neq\emptyset$。</p>
<p><strong>Nonnegative Matrix Factorization（非负矩阵分解）：</strong></p>
<p>​        给定$n$个节点和$K$个社区，我们定义社区隶属度矩阵$U_{n\times K}$，其中$U_{ij}$表示了第$i$个节点属于第$j$的概率。根据隶属度矩阵计算存在边的数量$UU^T$应该拟合通过邻接矩阵$A$得到的链接数。因此，基于最小化重建成本，可以将社区发现问题表示为(SNMF)：</p>
<script type="math/tex; mode=display">
\mathop{min}\limits_{U\geq0}||A-UU^T||_F^2\tag{1}</script><p>​        SNMF使用了上述公式（1），这有着一个严重缺陷，它通过分解邻接矩阵$A$只利用了网络的拓扑结构。对于每对相连的节点对$v_i$和$v_j$，无论它们是否属于同一个社区，$A_{ij}=1$。因此，固有的社区截割头不能通过邻接矩阵$A$来表示。为了解决这个缺陷，我们设计了新颖的社区结构嵌入方法。</p>
<h2 id="4-Community-Structure-Embedding"><a href="#4-Community-Structure-Embedding" class="headerlink" title="4. Community Structure Embedding"></a>4. Community Structure Embedding</h2><p>​        为了更好地描述图中固有的社区结构，我们提出了一种新颖的社区结构嵌入方法，根据节点潜在的社区隶属相似性来量化节点的结构紧密度。具体而言，我们设计了一个评估社区隶属相似度的函数$\cal F$。然后根据相似度指标，我们采用基于负采样的Skip-gram来探索网络结构并描述潜在的社区结构。最后我们获得了对固有社区结构编码的社区结构嵌入矩阵。</p>
<p><strong>Community Membership Similarity：</strong></p>
<p>​        我们从一个简洁合理的观察开始，因为节点在社区中紧密相连，所以每个链接的节点都应具有一定的属于同一社区的趋势。定义评估社区隶属相似度的函数${\cal F} (i,j)\in[0,1)$：</p>
<script type="math/tex; mode=display">
{\cal F}(i,j)=2\sigma(U_{i:}U_{j:}^T)-1=2\times(\frac{1}{1+e^{-U_{i:}U_{j:}^T}}-1)
\tag{2}</script><p>​        之所以选择sigmoid函数，是因为对于所有实数输入值，它都有界可微的实函数（real function）。因为${\cal F} (i,j)$只是$\sigma(U_{i:}U_{j:}^T)$的非线性变换，因此我们只讨论$\sigma(U_{i:}U_{j:}^T)$。</p>
<p><strong>Community Structure Embedding Matrix (M)：</strong></p>
<p>​        社区结构嵌入的方法主要是：最大化实际相连节点$v_i$和$v_j$的$\sigma(U_{i:}U_{j:}^T)$，同时最小化随机选择的两个节点</p>
<p>$v_i$和$v_j$的$\sigma(U_{i:}U_{j:}^T)$。在现实世界中，大多数大规模图都是非常稀疏的，随机选择的两个节点相连的概率是非常低的。因此，通过基于负采样的Skip-gram，定义目标函数如下：</p>
<script type="math/tex; mode=display">
{\cal l}(i,j)=W_{ij}\big(log\sigma(U_{i:}U_{j:}^T)+{k}{\Bbb E}_{j_N\sim P_V}[log\sigma(-U_{i:}U_{j_N:}^T)]\big)
\tag{3}</script><p>式中：$k$为负采样的样本数，$log\overset{\text{def}}{=}log_e$，$j_N$是根据经验分布$P_V(i)=\frac{d_i}{D}$，其中$d_i=\sum_jW_{ij}$是节点 $i$ 的度数，$D=\sum_id_i$是图$G$总度数。将上述内容带入公式（3）：</p>
<script type="math/tex; mode=display">
{\cal l}(i,j)=W_{ij}log\sigma(U_{i:}U_{j:}^T)+{k}\frac{d_id_j}{D}log\sigma(-U_{i:}U_{j_N:}^T)
\tag{4}</script><p>接下来对公式（4）求导：</p>
<script type="math/tex; mode=display">
\frac{\partial{l(i,j)}}{\partial({U_{i:}U_{j:}^T)}}=W_{ij}\sigma(-U_{i:}U_{j:}^T)-{k}\frac{d_id_j}{D}\sigma(U_{i:}U_{j_N:}^T)
\tag{5}</script><p>最后，通过将公式（5）=0，可以得到任意节点对的最优社区结构嵌入，并得出：</p>
<script type="math/tex; mode=display">
U_{i:}U_{j:}^T=log\frac{W_{ij}D}{d_id_j}-logk
\tag{6}</script><p>公式（6）可能是负的，如果为负，则设置为0。基于以上分析结果，我们定义一个新的社区结构嵌入矩阵：</p>
<script type="math/tex; mode=display">
M_{ij}=max\{U_{i:}U_{j:}^T,0\}=max\{log\frac{W_{ij}D}{d_id_j}-logk,0\}
\tag{7}</script><p>对于每对不相连的$W_{ij}=0$节点对$v_i$和$v_j$，我们设置相应的$M_{ij}=0$。</p>
<p>​        社区结构嵌入矩阵可以对潜在的紧密相连的子图进行编码，并探索固有的社区结构。此外，它可以容易地拓展到各种网络（有向无向，有权重无权重），有这广阔的应用场景。</p>
<p><strong>Example：</strong></p>
<p>​        举个简单例子，以证明社区结构嵌入矩阵$M$（k=2）在无权重图对固有社区结构编码的优越性能。图1（a）说明了有两个社区的无权重图，由于邻接矩阵$A$对每条边都设置了相同的值1，因此在发现两个社区时，无法确定那条边更重要。而如图1（b）社区结构嵌入矩阵可以为每个社区内的链接设置更高的权重，而为两个社区之间的链接设置更小的权重。特别是，社区结构嵌入矩阵降低了高度数节点间连接的权重，因为它们更可能在划分社区中发挥关键作用。如图1（a），节点$v_1$和节点$v_2$都有着最大的度数，因此删除（b）中边$(v_1,v_2)$，即$M_{v_1v_2}=0$。</p>
<p><strong>Structure Embedding based Optimization：</strong></p>
<p>​        不同于基于NMF的研究（如SNMF）直接通过邻接矩阵$A$利用图结构，本文使用社区结构嵌入矩阵来描述固有的社区结构。</p>
<p>​        不考虑属性时，使用社区结构嵌入矩阵进行社区发现的问题可以表示为：</p>
<script type="math/tex; mode=display">
\mathop{min}\limits_{U\geq0}{\cal L}(U)=||M-UU^T||_F^2
\tag{8}</script><h2 id="5-CDE-Model"><a href="#5-CDE-Model" class="headerlink" title="5. CDE Model"></a>5. CDE Model</h2><p>​        在本节中，我们提出CDE模型，来解决使用结构信息和节点属性信息对属性图进行社区发现的问题。</p>
<p><strong>Community Attributes：</strong></p>
<p>​        回想一下，在属性图中，社区不仅应具有紧密连接的结构，而且还应具有相同的属性。此外，不同的社区具有不同的属性。因此，我们定义社区属性矩阵$C\in R_{K\times s}$，其中$C_{ir}$表示第$i$个社区对第$r$维节点属性的偏向。</p>
<p>​        我们将社区隶属度和社区属性的发现公式转化为非负矩阵分解问题，通过对节点属性矩阵进行分解来优化社区隶属度矩阵$U$和社区属性矩阵$C$：</p>
<script type="math/tex; mode=display">
\mathop{min}\limits_{U\geq0,C\geq0}{\cal L}(C)=||T-UC||_F^2+\alpha\sum_i||C_{:i}||_1^2
\tag{9}</script><p>式中：$\alpha$是一个非负参数，来控制矩阵$C$的稀疏性。</p>
<p>​        因为不同的社区倾向于拥有不同的属性，甚至，某些属性因此会互斥。为了解决这个问题，我们在C的每一列中使用了l1范数稀疏性，这减少了每个社区的不重要节点属性的干扰。</p>
<p><strong>Unified Objective Function for CDE：</strong></p>
<p>​        在属性图$G$中，CDE的目标是找到K个社区，并且相同社区内的节点紧密相连且拥有相同的属性。通过将社区结构嵌入的目标函数和节点属性分别合并为公式（8）和公式（9），定义最终的CDE的目标函数如下：</p>
<script type="math/tex; mode=display">
\mathop{min}\limits_{U\geq0,C\geq0}{\cal L}(U,C)=||T-UC||_F^2+\alpha\sum_i||C_{:i}||_1^2+\beta||M-UU^T||_F^2
\tag{10}</script><p>式中：$\beta$是一个正参数，用于平衡节点属性矩阵$T$和社区结构嵌入矩阵$M$。更大的$\beta$会导致更依赖于社区结构嵌入矩阵$M$的分解来确定社区。</p>
<p><strong>Identify Communities by U：</strong></p>
<p>​        对于节点$v_i$，定义包含节点$v_i$的社区集合为$\psi(i)\subseteq\{1,…,K\}$。通过对公式（10）的优化后，我们获得了最佳的社区隶属度矩阵$U$。为了识别不重叠的社区，我们找出$v_i$对应的值最大的$U_{i:}$，即$\psi(i)=argmax_{1\leq j\leq K}U_{ij}$。对于重叠的社区发现问题，当$U_{ij}$大于某个阈值时，则认为节点$v_i$属于第$j$个社区，即$\psi(i)={1\leq j\leq K|U_{ij}&gt;\varepsilon}$。本文的实验中，设置阈值$\varepsilon-0.1$。</p>
<p><strong>Iteratively Updating Rules for CDE：</strong></p>
<p>​        由于目标函数（10）不是凸函数，我们采用了一个迭代更新规则。在每次迭代中，先固定$C$更新$U$，然后固定$U$更新$C$。</p>
<ol>
<li><p>首先固定$C$，更新$U$：</p>
<script type="math/tex; mode=display">
\mathop{min}\limits_{U\geq0}{\cal L}(U)=\beta||M-UU^T||_F^2+||T-UC||_F^2
\tag{11}</script><p>初始化$U$后，可以按下式对$U$进行更新：</p>
<script type="math/tex; mode=display">
U_{ij}\leftarrow U_{ij}(\frac{(TC^T-UCC^T+2\beta MU)_{ij}}{(2\beta UU^TU)_ij})^{\frac{1}{4}}
\tag{12}</script></li>
<li><p>固定$U$，更新$C$：</p>
<script type="math/tex; mode=display">
\mathop{min}\limits_{C\geq0}{\cal L}(C)=||T-UC||_F^2+\alpha\sum_i||C_{:i}||_1^2
\tag{13}</script><p>按下式对$C$进行更新：</p>
<script type="math/tex; mode=display">
C_{ij}\leftarrow C_{ij}\frac{(U'^TT')_{ij}}{(U'^TU'C)_{ij}}
\tag{15}</script></li>
</ol>
<p><strong>Proof of Convergence:</strong></p>
<p>…………………</p>
<h2 id="6-Experiments"><a href="#6-Experiments" class="headerlink" title="6. Experiments"></a>6. Experiments</h2><p><strong>数据集：</strong></p>
<p>数据集的详细信息如下图所示：</p>
<p><img src="https://dede-photo.oss-cn-beijing.aliyuncs.com/img/数据集信息.png" alt="数据集信息" style="zoom:50%;" /></p>
<p><strong>参数敏感性分析：</strong></p>
<p>​        在Wisconsin数据集上进行参数敏感性实验。CDE模型有着以下参数：</p>
<p>（1）$\alpha$：负，控制社区属性矩阵$C$的稀疏性</p>
<p>（2）$\beta$：正，平衡节点属性和社区结构嵌入的作用</p>
<p>（3）$k$：决定负采样的数量</p>
<p>​        首先设置$\alpha=\beta=1$，即认为节点属性和社区结构嵌入是同等重要，然后改变从1到100$k$，记录结果：</p>
<p><img src="https://dede-photo.oss-cn-beijing.aliyuncs.com/img/参数敏感性实验1.png" alt="参数敏感性实验1" style="zoom:50%;" /></p>
<p>可以观察到，$k=22$到$k=29$区间，得分较好。$k=25$时得分最高，AC=0.6645，NMI=0.409。</p>
<p>​        接下来，设置$k=25$，然后改变$\alpha$和$\beta$，结果如下图所示：</p>
<p><img src="https://dede-photo.oss-cn-beijing.aliyuncs.com/img/参数敏感性实验2.png" alt="参数敏感性实验2" style="zoom:50%;" /></p>
<p>可以看到，$\alpha=1,\beta=2$时，得分最高，AC=0.7321，NMI=0.4284。</p>
<p><strong>重叠社区评估：</strong></p>
<p>评估方法：每个被发现的社区匹配与它最相思的真实社区。给定被发现的社区集合$C$和现实存在的社区集合$C^*$，F1-Score和Jaccard的统一描述定义如下：</p>
<script type="math/tex; mode=display">
\frac{1}{2|C^*|}\sum_{C_i^*\in C^*}\mathop{max}\limits_{C_j\in C}\delta(C_i^*,C_j)+\frac{1}{2|C|}\sum_{C_j\in C}\mathop{max}\limits_{C_j^*\in C^*}\delta(C_i^*,C_j)</script><p>当$\delta(C_i^<em>,C_j)$ 定义为 $C_i^</em>$ 和 $C_j$ 的调和平均数（倒数平均数）时，即为F1-Score矩阵；当 $\delta(C_i^<em>,C_j)=\frac{C_i^</em>\cap C_j}{C_i^*\cup C_j}$ 时，即为Jaccard矩阵。两个矩阵都是更高的分数代表了更好的性能。实验结果如下图所示：</p>
<p><img src="https://dede-photo.oss-cn-beijing.aliyuncs.com/img/重叠社区实验.png" alt="重叠社区实验" style="zoom:50%;" /></p>
<p><strong>无重叠社区评估：</strong></p>
<p>评估方法：AC和NMI</p>
<p>实验结果如下图所示：</p>
<p><img src="https://dede-photo.oss-cn-beijing.aliyuncs.com/img/无重叠社区实验.png" alt="无重叠社区实验" style="zoom:50%;" /></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dymzcc.github.io/2020/08/20/Graph%20Attention%20Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="dede">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="三环三星">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/08/20/Graph%20Attention%20Networks/" class="post-title-link" itemprop="url">Graph Attention Networks</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-08-20 16:16:50 / 修改时间：18:29:08" itemprop="dateCreated datePublished" datetime="2020-08-20T16:16:50+08:00">2020-08-20</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Graph-Attention-Networks"><a href="#Graph-Attention-Networks" class="headerlink" title="Graph Attention Networks"></a>Graph Attention Networks</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>​        人类的注意力机制（Attention Mechanism）是从直觉中得到的，它是人类利用有限的注意力资源从大量的信息中快速筛选出高价值信息的手段。</p>
<p><img src="https://dede-photo.oss-cn-beijing.aliyuncs.com/img/人类的注意力机制.png" alt="人类的注意力机制" style="zoom:50%;" /></p>
<p>​        借鉴人类的注意力机制原理，将其运用到NLP，CV中，都已经取得了非常好的表现。 注意机制的好处之一是，它们允许处理大小可变的输入，着重于决策中输入最相关的部分。 当使用注意力机制来计算单个序列的表示时，通常将其称为自我注意力或内部注意力。 </p>
<p>注意力机制有趣的属性：</p>
<p>（1）计算速度快，可以在不同节点上进行并行操作</p>
<p>（2）通过对邻域节点指定权重，可以泛化应用到任意具有不同度的节点</p>
<p>（3）是一种inductive的模型，可以泛化到没有参与训练的节点</p>
<p>​        GAT是为图上的节点分类任务设计，思想很简单，GAT中目标节点的向量表示是受其他节点的影响的，因此通过注意力机制找出对目标节点影响更大的节点，让它们参与到目标节点的嵌入过程中去，使得向量表示更准确。</p>
<p>​        就好比是给一个人A作评价，然后需要找一些人对A作评价，根据这些人对A的评价，最后给出最终评价。如果随机找人来作评价肯定是不合适的，比较科学合适的方法是，找出A的亲朋好友，让他们来作评价，他们和A的关系更亲密，他们的评价也会更有参考价值，使最终的评价更准确。</p>
<h2 id="GAT-Architecture"><a href="#GAT-Architecture" class="headerlink" title="GAT Architecture"></a>GAT Architecture</h2><h3 id="Graph-Attention-Layer"><a href="#Graph-Attention-Layer" class="headerlink" title="Graph Attention Layer"></a>Graph Attention Layer</h3><p>​        该层的输入是一组节点的特征，h = {$\vec{h_1},\vec{h_2},…,\vec{h_N}$}，$\vec{h_N}\in R^F$，其中 N 是节点数，F 是节点的特征维数，该层会生成一组新的节点特征作为该层的输出，$h’={}${$\vec{h’_1},\vec{h’_2},…,\vec{h’_N}$}，$\vec{h’_N}\in R^{F’}$。</p>
<p>​        第一步，通过共享权重矩阵$W\in R^{F’\times F}$对每个节点参数做线性变换，即对节点特征的增维，然后引入注意力机制，$a:R^{F’}\times R^{F’}\to R$，计算<strong>初步注意力系数</strong>：</p>
<script type="math/tex; mode=display">
e_{ij}=a(W\vec H_i,W\vec H_j)
\tag{1}</script><p>$(,)$表示将两个向量进行拼接，$a$ 表示将拼接后的向量映射为一个实数。注意力系数表示了节点 $j$ 对节点 $i$ 的重要程度。得到初步注意力系数后，用Softmax进行归一化：</p>
<script type="math/tex; mode=display">
\alpha_{ij} = softmax_j(e_{ij})=\frac{exp(e_{ij})}{\sum_{k\in N_i}exp(e_{ik})}
\tag{2}</script><p>式中：$N_i$表示节点 $i$ 的邻域节点。</p>
<p>​        在本文的实验中，注意力机制$a$是由权重向量 $\vec a\in R^{2F’}$参数化单层前馈神经网络，并使用激活函数LeakyReLU（负输入斜率$\alpha=0.2$），完全展开后，<strong>最终注意力系数</strong>为：</p>
<script type="math/tex; mode=display">
\alpha_{ij}=\frac{exp(LeakyReLU(\vec a^T[W\vec h_i||W\vec h_j]))}{\sum_{k\in N_i}exp(LeakyReLU(\vec a^T[W\vec h_i||W\vec h_k]))}
\tag{3}</script><p>其中，$\cdot^T$表示转置，||表示向量连接，过程如下图所示：</p>
<p><img src="https://dede-photo.oss-cn-beijing.aliyuncs.com/img/注意力系数.png" alt="注意力系数" style="zoom:40%;" /></p>
<p>​        获得归一化的注意力系数后，把邻域节点的特征向量进行加权求和，最后将结果做一次非线性变换，以此作为每个节点最终的特征输出向量：</p>
<script type="math/tex; mode=display">
\vec {h'_i}=\sigma (\sum_{j\in N_i}\alpha_{ij}W\vec h_j)
\tag{4}</script><h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p>​        为了稳定自我注意力的学习过程，引入多头注意力机制。</p>
<h4 id="Concat："><a href="#Concat：" class="headerlink" title="Concat："></a>Concat：</h4><p>​        由K个独立的注意力机制执行等式4的转换，然后将它们的特征连接起来，从而得到以下输出特征表示：</p>
<script type="math/tex; mode=display">
\vec{h'_i}=||_{k=1}^K(\sigma(\sum_{j\in N_i}\alpha_{ij}^kW^k\vec h_j))
\tag{5}</script><p>​        其中，||为向量拼接操作，$\alpha_{ij}^k$是第$K$个注意力机制计算出的归一化注意力系数，$W^k$是相应的输入线性变化的权重矩阵。注意，在此设定下，最终返回的输出$h’$的维数为$KF’$。可以理解为多个卷积核的操作，每个注意力机制处理一部分的特征，最后将结果做一个拼接。</p>
<h4 id="Average："><a href="#Average：" class="headerlink" title="Average："></a>Average：</h4><p>​        如果我们在网络的最终层上进行多头关注，则向量拼接是没有意义的，因此采用求平均，最后做非线性变化：</p>
<script type="math/tex; mode=display">
\vec {h'_i}=\sigma(\frac{1}{K}\sum_{k=1}^K\sum_{j\in N_i}\alpha_{ij}^kW^k\vec h_j)
\tag{6}</script><p>下图说明了多头注意力机制的聚合过程：</p>
<p><img src="https://dede-photo.oss-cn-beijing.aliyuncs.com/img/多头注意力机制.png" alt="多头注意力机制" style="zoom:40%;" /></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dymzcc.github.io/2020/08/18/Overlapping%20Community%20Detectionwith%20Graph%20Neural%20Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="dede">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="三环三星">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/08/18/Overlapping%20Community%20Detectionwith%20Graph%20Neural%20Networks/" class="post-title-link" itemprop="url">Overlapping Community Detection with Graph Neural Networks</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-08-18 14:41:50" itemprop="dateCreated datePublished" datetime="2020-08-18T14:41:50+08:00">2020-08-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-10-06 20:59:37" itemprop="dateModified" datetime="2020-10-06T20:59:37+08:00">2020-10-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Overlapping-Community-Detection-with-Graph-Neural-Networks"><a href="#Overlapping-Community-Detection-with-Graph-Neural-Networks" class="headerlink" title="Overlapping Community Detection with Graph Neural Networks"></a>Overlapping Community Detection with Graph Neural Networks</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>​        图提供了真实复杂世界系统的一种自然的表达方式。社区发现方法是理解这些系统的结构和行为的重要工具。社区发现问题引起了研究社区的广泛关注，并提出了许多模型和算法。</p>
<p>​        近年来，图上的深度学习已经取得了令人瞩目的成效，并在图相关任务（如链接预测，节点分类）中取得了空前的成果，但无监督社区发现任务却少有人关注。目前已经提出了几种方法，但是他们都有一个共同的缺点：只关注不重叠社区的特殊情况。然而，现实中的社区大部分都是重叠的，处理重叠的社区发现问题是现有的深度学习方法无法满足的。在本文中，我们解决了这一研究空白，并提出了一种能够检测重叠社区的深度学习模型。</p>
<h2 id="相关定义"><a href="#相关定义" class="headerlink" title="相关定义"></a>相关定义</h2><p>$G$表示无向无权重图，$A\in\{0,1\}^{N\times N}$表示邻接矩阵</p>
<p>$V=\{1,…,N\}$表示点集，$N$为节点数，$E=\{(u,v)\in V\times V:A_{uv}=1\}$表示边集，$M$为边数</p>
<p>每个节点对应一个$D$维属性向量，所有节点可以用属性矩阵$X\in \Bbb R^{N\times D}$表示</p>
<p>重叠社区发现的目标是将节点分配到$C$个社区，可以通过节点隶属度矩阵$F\in \Bbb R^{N\times C}_{\geq 0}$来分配，其中$F_{uc}$表示节点$u$对社区$c$的隶属度。</p>
<p>“社区”没有一个统一的定义，本文采用这样的定义：社区是一组节点，与图上的其他节点相比，社区内节点之间连接形成边的概率更大。这样，可以根据概率推断框架来处理社区发现问题。一旦为图建立了基于社区的生成模型$p(G|F)$，社区发现问题就可以归结为给定图$G$，推断出隶属关系矩阵$F$。</p>
<h2 id="NOCD模型"><a href="#NOCD模型" class="headerlink" title="NOCD模型"></a>NOCD模型</h2><h3 id="1-BP模型"><a href="#1-BP模型" class="headerlink" title="1. BP模型"></a>1. BP模型</h3><p>​        Bernoulli-Poisson（BP）是一个图生成式模型，能够处理重叠的社区。根据BP模型，图的生成如下所示：给定隶属关系矩阵$F\in\Bbb R^{N\times C}_{\geq0}$，邻接矩阵中$A_{uv}$按下式进行采样：</p>
<script type="math/tex; mode=display">
A_{uv}\sim Bernoulli(1-exp(-F_uF_v^T))
\tag{1}</script><p>式中$F_u$是节点$u$对应的社区隶属度向量，上式定义了两个顶点$u$和$v$之间存在边的概率，可以理解为图中真实存在的边是根据这些两两之间相连的概率采样得到的。<strong>直观地，节点$u$和$v$之间共享的社区越多，它们之间存在边的概率越大</strong>。该模型有许多理想的属性：（1）可以产生各种社区拓扑结构，生成密集的重叠社区（2）计算高效。</p>
<h3 id="2-模型定义"><a href="#2-模型定义" class="headerlink" title="2. 模型定义"></a>2. 模型定义</h3><p>​        本文中没有把$F$作为自由变量去优化，即没有像node2vec，LINE那样直接优化隶属度向量$F_u$的向量表达。而是通过GNN来生成隶属度矩阵$F$，通过训练参数矩阵$\theta$来优化隶属度向量$F_u$的向量表达：</p>
<script type="math/tex; mode=display">
F:=GNN_{\theta}(A,X)
\tag{2}</script><p>式中：A为邻接矩阵，X为特征矩阵</p>
<p>​        在本文的实验中，GNN为2层的GCN，GCN定义如下：</p>
<script type="math/tex; mode=display">
F:=GCN_{\theta}(A,X)=ReLU\big(\hat AReLU(\hat AXW^{(1)})W^{(2)}\big)
\tag{3}</script><p>式中$\hat A=\tilde D^{-1/2}\tilde A\tilde D^{-1/2}$是归一化的邻接矩阵，$\tilde A=A+I_N$是邻接矩阵$A$加上一个单位矩阵，$\tilde D$表示$\tilde A$的度矩阵。</p>
<p>​        输入特征矩阵$X$和邻接矩阵$A$，然后经过两层GCN，得到隶属度矩阵$F$，如果节点的特征属性$X$不可用，可以把$A$作为节点特征进行输入。为什么进行这个操作？后面作者也说了，实验结果说明了一切。</p>
<p>​        定义了隶属度矩阵$F$的表达后，接下来要定义损失函数，来优化GCN的参数矩阵，最终优化$F$的向量表达。首先定义BP模型的负对数似然（negative log-likelihood）：</p>
<script type="math/tex; mode=display">
-logp(A|F)=-\sum_{(u,v)\in E}log\big(1-exp(-F_uF_v^T)\big)+\sum_{(u,v)\notin E}F_uF_v^T
\tag{4}</script><p>​        这个可以理解为，确定了每个节点的隶属度向量，即确定了每个节点对各个社区的隶属概率后，对图中所有节点两两相连的概率进行计算，即确定了$F$后确定邻接矩阵$A$的条件概率：$-logp(A|F)$。由于真实图是比较稀疏的，这会导致第二项 $\sum_{(u,v)\notin E}F_uF_v^T$会对损失的影响更大，因此使用平衡分类的技术来定义损失函数：</p>
<script type="math/tex; mode=display">
\cal L(\it F)=-\Bbb E_{(u,v)\sim P_E}\rm [log\big(1-exp(-F_uF_\it v^T\rm)\big)]+\Bbb E_{(u,\it v\rm)\sim P_N}[F_uF_\it v^T\rm]
\tag{5}</script><p>实验没有对隶属矩阵$F$进行直接优化，而是通过确定最优的参数$\theta^{\star}$来最小化负对数似然：</p>
<script type="math/tex; mode=display">
\theta^{\star}=\mathop{argmin}\limits_{\theta}\cal L\big(\rm GNN_{\theta}(A,X)\big)
\tag{6}</script>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dymzcc.github.io/2020/08/18/GraphGAN-Graph-Representation-Learning-with-Generative-Adversarial-Nets/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="dede">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="三环三星">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/08/18/GraphGAN-Graph-Representation-Learning-with-Generative-Adversarial-Nets/" class="post-title-link" itemprop="url">GraphGAN: Graph Representation Learning with Generative Adversarial Nets</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-08-18 00:54:50" itemprop="dateCreated datePublished" datetime="2020-08-18T00:54:50+08:00">2020-08-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-10-06 20:59:52" itemprop="dateModified" datetime="2020-10-06T20:59:52+08:00">2020-10-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="GraphGAN-Graph-Representation-Learning-with-Generative-Adversarial-Nets"><a href="#GraphGAN-Graph-Representation-Learning-with-Generative-Adversarial-Nets" class="headerlink" title="GraphGAN: Graph Representation Learning with Generative Adversarial Nets"></a>GraphGAN: Graph Representation Learning with Generative Adversarial Nets</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>​        大多数的图嵌入算法可以分为两类：生成式的图嵌入算法，判别式的图嵌入算法</p>
<p><strong>生成式的图嵌入算法：</strong>生成式模型假设对每个节点$v_c$，都存在着一种真实的链接分布$p_{true}(v|v_c)$，这表明了节点$v_c$在整个网络中的连接偏向（可以理解为，删除图中所有边，现在节点$v_c$要选择与一个节点$v$相连，图中剩余的节点都有一定的概率与$v_c$相连，这个概率分布即为$p_{true}(v|v_c)$）。因此，图中的边可以看做是根据$p_{true}(v|v_c)$采样所生成的样本（即根据概率分布采样），并且这些生成模型通过最大化图中边存在的可能性来学习节点嵌入。</p>
<p><strong>判别式的图嵌入算法：</strong>旨在学习得到一个可以直接预测边是否存在的分类器。典型地，判别式模型把两个节点$v_i$和$v_j$视为特征，并预测两个顶点间存在边的概率，即$p_(edge|(v_i,v_j))$。</p>
<p>​        尽管生成式模型和判别式模型是两类图表示学习方法，但是两者可以看做是同个硬币的两面，相互对抗，相互联系。受到GAN的启发，本文提出了GraphGAN，一种结合了生成模型和判别模型的新颖的框架。在GraphGAN中，生成模型和判别模型在minmax中进行博弈竞争，生成模型旨在生成更难以区分的”假“节点，判别模型旨在更准确地区分“真假”节点。</p>
<p>（1）生成模型$G(v|v_c)$：拟合真实的链接分布$p_{true}(v|v_c)$，生成$v_c$最有可能连接的顶点。</p>
<p>（2）判别模型$D(v,v_c)$：预测$v$和$v_c$之间存在边的概率。</p>
<p>​        在实验过程中，我们发现传统的softmax函数并不适用于生成模型，理由如下：（1）对于给定顶点，softmax对图中的其他顶点一视同仁，而缺少对图结构信息和近邻信息的考虑。（2）softmax函数涉及图中所有顶点，计算量非常大。</p>
<p>​        为了克服这个限制，我们在GraphGAN中提出了一种新的生成器的实现方式：Graph Softmax。Graph Softmax对图中的链接分布做了新的定义，我们证明了Graph Softmax满足规范化（Normalization），图结构意识（Graph Structure Awareness），计算效率（Computational Efficiency）的理想特性。因此，我们提出了一种基于随机游走的在线生成策略的生成器，它与Graph Softmax定义一致，可以大大降低计算复杂度。</p>
<h2 id="Graph-Generative-Adversarial-Nets"><a href="#Graph-Generative-Adversarial-Nets" class="headerlink" title="Graph Generative Adversarial Nets"></a>Graph Generative Adversarial Nets</h2><h3 id="GraphGAN-Framework"><a href="#GraphGAN-Framework" class="headerlink" title="GraphGAN Framework"></a>GraphGAN Framework</h3><p><strong>符号定义：</strong></p>
<p>$\cal G=(V,E)$表示一张图，其中$\cal V=\{v_1,…,v_V\}$表示节点集合，$\cal E=\{e_{ij}\}_{i,j=1}^V$表示边集合。</p>
<p>对于一个给定的节点$v_c$，定义$N(v_c)$为与$v_c$直接相连的节点（即一阶邻域）。</p>
<p>定义节点$v_c$的潜在真实链接分布为$p_{true}(v|v_c)$，这反映了节点$v_c$倾向于哪些$\cal V$中顶点相连。$\cal N(v_c)$可以看做是根据$p_{true}(v|v_c)$采样得到。</p>
<p><strong>生成模型</strong>$G(v|v_c;\theta_G)$：拟合真实的链接分布$p_{true}(v|v_c)$，生成$v_c$最有可能连接的节点。</p>
<p><strong>判别模型</strong>$D(v,v_c;\theta_D)$：区分顶点对$v,v_c$之间的连接性，输出$v$和$v_c$之间存在边的概率。</p>
<p>​        生成器$G$会拟合真实的链接分布$p_{true}(v|v_c)$，尽力去生成与节点$v_c$实际连接的邻域节点最相似的生成节点，尽力让判别器$D$认为生成的节点就是$v_c$实际的邻域节点；而判别模型$D$则为尽力去分辨这些节点是真实的$v_c$的邻域节点，还是由对手生成模型$G$生成的节点。因此，$G$和$D$实际即在minmax中竞争博弈：</p>
<script type="math/tex; mode=display">
\mathop{min}\limits_{\theta _G}\mathop{max}\limits_{\theta _D}V(G,D)=\sum_{c=1}^{V}\left(\Bbb E_{v\sim p_{true}(\cdot|v_c)}\left[logD(v,v_c;\theta_D) \right]+ \Bbb E_{v\sim G(\cdot|v_c;\theta_G) }\left[log\left(1-D(v,v_c;\theta_D)\right) \right]\right)\tag{1}</script><p>​        GraphGAN框架如下图所示：</p>
<p><img src="https://dede-photo.oss-cn-beijing.aliyuncs.com/img/image-20200806092820648.png" alt="image-20200806092820648" style="zoom:70%;" /></p>
<p>​        在每一次迭代中，判别器$D$用来自真实分布$p_{true}(\cdot|v_c)$的正样本（绿色顶点）和来自生成器$D(\cdot,v_c;\theta_D)$生成的负样本（蓝色样本）训练，生成器$G$则在判别器$D$的反馈指导下，按照梯度政策进行更新。生成器$G$和判别器$D$之间的竞争博弈，会提高各自的性能，直到判别器$D$无法区分真实节点和生成节点。</p>
<h3 id="（1）-G-和-D-简单的实现"><a href="#（1）-G-和-D-简单的实现" class="headerlink" title="（1）$G$和$D$简单的实现"></a>（1）$G$和$D$简单的实现</h3><p>​        对于给定的来自真实分布的正样本和来自生成生成的负样本，判别器$D$旨在最大化将正负样本对应正确标签的概率，这可以通过随机梯度上升来解决。判别器$D$定义如下：</p>
<script type="math/tex; mode=display">
D(v,v_c)=\sigma(d_v^Td_{v_c})=\frac{1}{1+exp(-d_v^Td_{v_c})}</script><p>式中：$d_v,d_{v_c}\in \Bbb R^k$表示节点$v$和$v_c$在判别器$D$中的k维向量表示，$\theta_D$可以看做是所有节点表示向量的向量空间（参数空间）。</p>
<p>​        一种生成器的直接定义是使用softmax函数，如下：</p>
<script type="math/tex; mode=display">
G(v|v_c)=\frac{exp(\rm g_\it v^\sf T\rm g_{\it v_c})}{\sum_{v\neq v_c}exp(\rm g_\it v^\sf T\rm g_{\it v_c})}
\tag{5}</script><p>式中，$\rm g_\it v,\rm g_{\it v_c}\in \Bbb R^k$表示生成器中$v$，$v_c$的k维表示向量，$\theta_G$可以看做是所有节点表示向量的向量空间</p>
<p>（参数空间）。在每次迭代中，为了更新$\theta_G$，首先根据公式（5）计算得到预估的链接分布$G(v|v_c)$，然后根据这个概率值进行随机采样可以得到样本集合$(v,v_c)$，然后用随机梯度下降更新$\theta_G$。</p>
<p>​        softmax为$G$中的链接分布提供了简洁直观的定义，但是它在图表示学习中存在中两个限制：（1）涉及图中所有节点，计算非常复杂（2）没有利用图的拓扑结构信息。</p>
<h3 id="（2）基于Graph-Softmax-的实现"><a href="#（2）基于Graph-Softmax-的实现" class="headerlink" title="（2）基于Graph Softmax 的实现"></a>（2）基于Graph Softmax 的实现</h3><p>​        为了解决上述问题，我们提出了Graph Softmax方法，该方法满足：<br>（1）Normalized：满足有效的概率分布$\sum_{v\neq v_c}G(v|v_c;\theta_G)=1$</p>
<p>（2） Graph-structu-aware：利用图的结构信息。直观的意思就是，对于两个节点，它们最短距离短小，连接概率越大。</p>
<p>（3）Computationally efficient：计算$G(v|v_c;\theta_G)$时，无需计算全部节点，减少计算量。</p>
<p>​        首先从$v_c$开始进行BFS，生成根结点为$v_c$BFS树$T_c$。给定$T_c$，我们定义$N_c(v)$为顶点$v$在树$T_c$的一阶邻域，包括父母节点和子节点。对于给定的一个节点$v$和其邻域节点$v_i\in N_c(v)$，定义条件概率如下：</p>
<script type="math/tex; mode=display">
p_c(v_i|v)=\frac{exp(\rm g_{\it v_i}^\sf T\rm g_\it v)}{\sum_{v_j\in N_c(v)}exp(\rm g_{\it v_j}^\sf T\rm g_\it v)}\tag{6}</script><p>​        对于计算$G(v|v_c;\theta_G)$，图中的每个节点$v$都可以看做是从根节点$v_c$出发的一条$T_c$树上的路径，该路径定义为：$p_{v_c\to v}=(v_{r_0},v_{r_1},…,v_{r_m})$，其中$v_{r_0}=v_c$，$v_{r_m}=v$。在Graph Softmax中$G(v|v_c;\theta_G)$可以定义为：</p>
<script type="math/tex; mode=display">
G(v|v_c;\theta_G)\overset{\Delta}{=}\Big(\prod_{j=1}^{m}p_c(v_{r_j}|v_{r_{j-1}})\Big)\cdot p_c(v_{r_{m-1}}|v_{r_m})
\tag{7}</script><p>式中：$p_c(\cdot|\cdot)$即为公式（6）中定义的条件概率。</p>
<p>​        一种简单的生成节点的方法就是根据所有节点$G(v|v_c;\theta_G)$进行有权重偏向的随机采样。我们提出了另一种计算更高效的生成方法，我们从根顶点$v_c$出发依据（6）中计算的概率进行有偏向的随机游走，如果当前游走到的节点 $v$的下一步是要游走回父母节点，则 $v$即被选择为采样的节点，图示如下：</p>
<p><img src="https://dede-photo.oss-cn-beijing.aliyuncs.com/img/image-20200806173118934.png" alt="image-20200806173118934"></p>
<p>生成器算法流程如下所示：</p>
<p><img src="https://dede-photo.oss-cn-beijing.aliyuncs.com/img/image-20200806174143751.png" alt="image-20200806174143751" style="zoom:50%;" /></p>
<p>GraphGAN算法流程如下所示：</p>
<p><img src="https://dede-photo.oss-cn-beijing.aliyuncs.com/img/image-20200806174216985.png" alt="image-20200806174216985" style="zoom:50%;" /></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dymzcc.github.io/2020/08/18/CommunityGAN-Community-Detection-with-Generative-Adversarial-Nets/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="dede">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="三环三星">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/08/18/CommunityGAN-Community-Detection-with-Generative-Adversarial-Nets/" class="post-title-link" itemprop="url">CommunityGAN: Community Detection with Generative Adversarial Nets</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-08-18 00:20:16" itemprop="dateCreated datePublished" datetime="2020-08-18T00:20:16+08:00">2020-08-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-10-06 20:59:44" itemprop="dateModified" datetime="2020-10-06T20:59:44+08:00">2020-10-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="CommunityGAN-Community-Detection-with-Generative-Adversarial-Nets"><a href="#CommunityGAN-Community-Detection-with-Generative-Adversarial-Nets" class="headerlink" title="CommunityGAN: Community Detection with Generative Adversarial Nets"></a>CommunityGAN: Community Detection with Generative Adversarial Nets</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>​        社区发现旨在找出网络中具有相似属性或者功能的节点群，如查找社交网络中属于相同组织的用户，生物医学网络中功能相似的蛋白质，引用网络中相同学科的论文集。</p>
<p>​        近年来，随着深度学习和网络表示学习的发展，这些技术也被应用到社区发现问题中来。但是问题在于，现实的社区大部分都是重叠的，一个顶点可能会属于数十个社区，而通过网络表示学习获得顶点向量后，通常采用聚类的方法进行社区发现，而大多数的聚类算法是无法处理密集的社区重叠问题。</p>
<p>​        本文提出了CommunityGAN，一种新颖的社区发现框架，可以用于解决重叠的社区发现和图嵌入学习问题。</p>
<h3 id="CommunityGAN-Framework"><a href="#CommunityGAN-Framework" class="headerlink" title="CommunityGAN Framework"></a>CommunityGAN Framework</h3><p>$\cal G=(\cal V,\cal E,\cal M)$表示一张图，其中$\cal V=\{v_1,…,v_V\}$是点集，$\cal E=\{e_{ij}\}_{i,j=1}^V$是边集。</p>
<p>$\cal M$是图 $\cal G$ 的motif集合，本文中只关注一种特殊的motif：clique。</p>
<p>给定一个顶点$v_c$，定义$M(v_c)$为图中包含$v_c$顶点的motif集合，$M(v_c)$集合的规模远小于$\cal V$。</p>
<p>定义条件概率$p_{true}(m|v_c)$为包含$v_c$顶点的motif集合在全体motif集合$\cal M$中的偏向分布。</p>
<p>因此，$M(v_c)$可以视为根据$p_{true}(m|v_c)$采样到的的motif。给定一张图$\cal G$，我们的目标是学习得到两个模型：</p>
<p>（1）Generator $G(s|v_c;\theta_G)$：通过近似$p_{true}(m|v_c)$，生成（或选择）包含$v_c$的集合，使该集合与现实的motif最相似。</p>
<p>（2）Discriminator $D(s,\theta_D)$：目的是输出顶点集合是真实motif（即来自$\cal M$）的概率。</p>
<p>​        生成器$G$和判别器$D$在minmax中相互博弈，生成器$G$会尽力近似$p_{true}(m|v_c)$，生成包含$v_c$的最接近真实motif的顶点集。判别器 $D$会尽力区分真实motif的$p_{true}(m|v_c)$和Generator $G$生成的motif之间的区别。正式地，$G$和$D$在minmax游戏中成为博弈的两个对手，公式如下：</p>
<script type="math/tex; mode=display">
\mathop{min}\limits_{\theta _G}\mathop{max}\limits_{\theta _D}V(G,D)=\sum_{c=1}^{V}\left(\Bbb E_{m\sim p_{true}(\cdot|v_c)}\left[logD(m;\theta_D) \right]+ \Bbb E_{s\sim G(s|v_c;\theta_G) }\left[log\left(1-D(s;\theta_D)\right) \right]\right)\tag{1}</script><ul>
<li>对于$\theta_D$，判别器希望自身能够分辨准确，对真实的样本$m\sim p_{true}(\cdot|v_c)$，让其概率值大，即最大$log(D(m;\theta_D))$；对于生成的样本$s\sim G(\cdot|v_c;\theta_G)$，让其概率值最小，即最大$log(1-D(s;\theta_D))$</li>
<li>对于$\theta_G$，生成器希望自身生成的样本能够骗过判别器，使其无法分辨出生成的样本，因此要让生成样本的概率最大，即最小$log(1-D(s;\theta_D))$</li>
</ul>
<p>基于公式（1），可以通过交替地最大化（$D$）最小化（$G$）值函数$V(G,D)$来学习最优的$G$和$D$。CommunityGAN的框架如下图所示：</p>
<p><img src="https://dede-photo.oss-cn-beijing.aliyuncs.com/img/image-20200802102116278.png" alt="image-20200802102116278" style="zoom:50%;" /></p>
<p>​        判别器 $D$ 通过$p_{true}(m|v_c)$中的正样本和$G(\cdot|v_c;\theta_G)$中的负样本来进行训练，并在$D$的指导下使用策略梯度技术更新生成器 $G$ 。$G$和$D$之间的竞争推动了二者的性能，直到$G$和真实的motif难以区分。</p>
<p>​        生成器和判别器的博弈就好比是造假团伙与银行之间的博弈，造假团伙希望造出更像的真钞的假钞，而银行则要鉴别出真钞和假钞，在这个博弈的过程中，造假团伙提升了自己造假的能力，银行提升了鉴别的能力。</p>
<h3 id="（1）-G-和-D-简单的实现"><a href="#（1）-G-和-D-简单的实现" class="headerlink" title="（1）$G$和$D$简单的实现"></a>（1）$G$和$D$简单的实现</h3><p>​        判别器的简单实现基于sigmoid函数，生成器的简单实现基于softmax函数。</p>
<p>​        对于判别器$D$，可以定义为输入顶点集合中，每两个顶点的内积做非线性变换后的乘积：</p>
<script type="math/tex; mode=display">
D(s)=\prod_{(u,v)\in s,u\neq v}\sigma(d_u^Td_v)\tag{4}</script><p>式中：$d_u,d_v\in\Bbb R^k$是判别器中顶点$u$和$v$的k维表示向量，$\theta_D$是判别器对于顶点所生成的空间。</p>
<p>​        对于生成器$G$，为了生成包含$v_c$的顶点集合，我们可以把$(v_{s_1},v_{s_2},…,v_{s_m})$这一顶点序列作为集合，其中$v_{s_1}=v_c$。因此，生成器$G$可以定义为：</p>
<script type="math/tex; mode=display">
G(s|v_c)=G_v(v_{s_2}|v_{s_1})G_v(v_{s_3}|v_{s_1},v_{s_2})...G_v(v_{s_m}|v_{s_1},...,v_{s_{m-1}})\tag{5}</script><p>​        值得注意的是，在公式（5）中，$v_{s_m}$的生成是基于$v_{s_1}$到$v_{s_{m-1}}$，而不是仅基于$v_{s_{m-1}}$。因为如果我们仅基于$v_{s_{m-1}}$生成$v_{s_m}$，很有可能就会导致$v_{s_m}$和其他顶点会属于不同的社区。生成器$G_v$定义如下：</p>
<script type="math/tex; mode=display">
G_v(v_{s_m}|v_{s_1},...,v_{s_{m-1}})=\frac{exp(\sum_{i=1}^{m-1}\rm g_{\it v_{s_m}}^{\sf T}g_{\it v_{s_i}})}{\sum_{v\notin (v_{s_1},...,v_{s_{m-1}})}exp(\sum_{i=1}^{m-1}\rm g_\it v^{\sf T}g_{\it v_{s_i}})}\tag{6}</script><p>式中：$\rm g_\it v\in\Bbb R^k$是生成器中顶点$v$的k维表示向量，$\theta_G$是生成器对于顶点所生成的空间。</p>
<h3 id="（2）基于Graph-AGM的初步实现"><a href="#（2）基于Graph-AGM的初步实现" class="headerlink" title="（2）基于Graph AGM的初步实现"></a>（2）基于Graph AGM的初步实现</h3><p>​        Sigmoid和Softmax函数为判别器和生成器给定了简洁直观的定义，但在社区发现任务中存在三个限制：</p>
<p>（1）在根据公式（4）和（6）学习到顶点的表示向量后，为了进行社区发现，我们仍然需要采用一些聚类算法，而大多数的聚类算法无法处理社区重叠的问题。</p>
<p>（2）公式（6）中的softmax的计算涉及图中的所有顶点，计算量大，效率低下。 </p>
<p>（3）没有充分利用图的拓扑结构信息。</p>
<p>​        为了解决这些问题，在CommunityGAN中，我们提出了一种新颖的判别器，生成器实现方式：Graph AGM（Affiliation Graph Model，隶属关系图模型）。</p>
<p><img src="https://dede-photo.oss-cn-beijing.aliyuncs.com/img/image-20200802144920051.png" alt="image-20200802144920051" style="zoom:50%;" /></p>
<p>​        在AGM中，每个顶点可以隶属于0个，1个，或者多个社区。如果顶点$u$隶属于社区$c$，则该隶属度不为负。</p>
<p>​        对于任意社区$c\in C$，其成员顶点$u$和$v$连接的概率为：$1-exp(-F_{uc}\cdot F_{vc})$。此外，每个社区$c$生成边是独立的。如果顶点$u$和$v$通过多个社区连接多次，则连接概率为：$1-exp(-\sum_cF_{uc}\cdot F_{vc})$。因此，顶点$u$和$v$（通过任何可能的社区）连接的概率为$p(u,v)=1-exp(-F_u^{\sf T}\cdot F_v)$，其中$F_u$和$F_v$是顶点$u$和$v$的非负$C$维隶属度向量。</p>
<p>​        接下来，我们拓展 edge generation到motif generation。</p>
<p>​        对于$m$个顶点$v_1$到$v_m$，定义通过社区$c$成为clique的概率为：$p_c(v_1,v_2,…,v_m)=1-exp(-\prod_{i=1}^mF_{v_ic})$，同时可以通过下式计算这$m$个顶点通过任意社区成为clique的概率：</p>
<script type="math/tex; mode=display">
\begin{align}
p(v_1,v_2,...,v_m)=&1-\prod_c(1-p_c(v_1,v_2,...,v_m))\\
=&1-exp(-\odot(F_{v_1},F_{v_2},...,F_{v_m}))
\end{align}
\tag{7}</script><p>式中，$\odot(F_{v_1},F_{v_2},…,F_{v_m})=\sum_{c=1}^C\prod_{i=1}^mF_{v_ic}$</p>
<p>​        判别器可以如下定义：</p>
<script type="math/tex; mode=display">
D(s)=1-exp(-\odot\big(d_{v_1},d_{v_2},...,d_{v_m})\big)</script><p>​        式中，$d_v\in \Bbb R^C$表示判别器中顶点$v$的表示向量，$\theta_D$表示判别器对顶点所生成的向量空间</p>
<p>接下来，生成器$G_v$可以进一步定义为：</p>
<script type="math/tex; mode=display">
G_v(v_{s_m}|v_{s_1},...,v_{s_{m-1}})=\frac{1-exp(-\odot(\rm g_{\it v_{s_1}},...,g_{\it v_{s_m}}))}{\sum_{v\notin v_{s_1},...,v_{s_{m-1}}}1-\rm exp\big(-\odot(\rm g_{\it v_{s_1}},...,g_{\it v_{s_{m-1}}},g_\it v\rm)\big)}</script><p>式中：$\rm g_\it v\in \Bbb R^C$是生成器$G_v$中顶点$v$的非负$C$维表示向量，$\theta_G$是生成器对于顶点所生成的空间。在此设置下，学习到的向量$\rm g_\cal v$可以表示顶点$v$和社区之间的隶属度，这意味着我们无需其他聚类算法就可以找到社区，因此就克服了第一个限制。</p>
<h2 id="（3）基于Graph-AGM的改进实现"><a href="#（3）基于Graph-AGM的改进实现" class="headerlink" title="（3）基于Graph AGM的改进实现"></a>（3）基于Graph AGM的改进实现</h2><p>​        为了进一步客服其他两个限制，受到《GraphGAN: Graph Representation Learning With Generative Adversarial Nets》的启发，我们如下设置了AGM：</p>
<p>​        为了计算$G_v(v_{s_m}|v_{s_1},…,v_{s_{m-1}})$，我们首先假设存在虚拟顶点$v_v$与$v_{s_1}$到$v_{s_{m-1}}$的<strong>全体邻域</strong>中的顶点相连，即$N(v_v)=N(v_{s_1})\bigcup…\bigcup N(v_{s_{m-1}})$，$N(v)$表示顶点$v$的邻域节点。此外，虚拟顶点$v_v$的表示向量$\rm g_{\it v_v}=g_{\it v_{s_1}}\circ\cdot\cdot\cdot\circ g_{\it v_{s_{m-1}}}$，即$v_v$的表示向量是$v_{s_1}$到$v_{s_{m-1}}$表示向量的逐项乘积。为了简单起见，我们用$G_v(s_m|v_v)$表示$G_v(v_{s_m}|v_{s_1},…v_{s_{m-1}})$。然后我们根据计算好的概率分布，从虚拟顶点$v_v$开始有偏向的随机游走。在随机游走过程中，如果当前的访问顶点是$v$，并且生成器$G$决定访问$v$的先前顶点，则选择$v$作为生成节点，并停止随机游走，下图说明了生成大小为3的顶点子集的过程：</p>
<p><img src="https://dede-photo.oss-cn-beijing.aliyuncs.com/img/image-20200803092010255.png" alt="image-20200803092010255"></p>
<p>​        此外，在随机游走过程中，我们希望游走路径总是与根顶点$v_v$相关，以此来最大化生成顶点集合成为motif的概率。因此，对于给定一个顶点$v_c$和它的一个邻域顶点$v_i\in N(v_c)$，我们定义给定$v_c$的$v_i$的相关概率为：</p>
<script type="math/tex; mode=display">
\ p_{v_v}(v_i|v_c)=\frac{1-exp(-\odot(\rm g_\it {v_i},\rm g_\it {v_c},\rm g_\it {v_v}\rm))}{\sum_{v_j\in N(v_c)}1-exp(-\odot(\rm g_\it {v_j},\rm g_\it {v_c},\rm g_\it {v_v}\rm))}\tag{10}</script><p>​        如果我们定义随机游走的路径为$P_r=(v_{r_1},v_{r_2},…,v_{r_n})$，则选择这条路径的概率为</p>
<p>$p_{v_v}(v_{r_{n-1}}|v_{r_n})\cdot \prod_{i=1}^{n-1}p_{v_v}(v_{r_{i+1}}|v_{r_i})$</p>
<p>在梯度策略中，我们通过选择路径，目标是从$D$中获得最大回报。因此，尽管从$v_{r_1}$到$v_{r_n}$有多条路径，如果我们已经选择了路径$P_r$，我们会优化该路径上的策略梯度，而忽略其他路径，换句话说，如果我们选择了路径$P_r$，我们定义$G_v(v_{s_m}|v_v)$，如下：</p>
<script type="math/tex; mode=display">
G_v(v_{s_m}|v_v)=p_{v_v}(v_{r_{n-1}}|v_{r_n})\cdot \prod_{i=1}^{n-1}p_{v_v}(v_{r_{i+1}}|v_{r_i})</script><p>​        最后，CommunityGAN的算法流程如下：</p>
<p><img src="https://dede-photo.oss-cn-beijing.aliyuncs.com/img/image-20200803093558119.png" alt="image-20200803093558119" style="zoom:50%;" /></p>
<h3 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h3><p><strong>模型初始化：</strong>我们有两种方法来进行模型初始化（1）我们利用AGM模型去学习获得顶点$v_i$对应的隶属度向量$F_i$，然后直接设置$g_{v_i}=d_{v_i}=F_i$（2）使用局部最小邻域，我们可以将每个顶点$v_i$及其邻域$N(v_i)$看做一个社区（$C(v_i)$）？？？？？</p>
<p><strong>确定社区成员：</strong>在学习参数$\theta_G$和$\theta_D$后，我们需要去决定每个顶点的社区成员身份，我们通过设置一个阈值。如果两个顶点属于同一个社区，则它们之间通过社区连接的概率应该大于background edge probability（$\epsilon=\frac{2E}{V(V-1)}$），于是我们设置这个阈值$\delta=\sqrt{-log(1-\epsilon)}$，如果$g_{v_{ic}}\geq \delta$或者$d_{v_{ic}}\geq \delta$，则认为顶点$v_i$属于社区$c$。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">dede</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">17</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">dede</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
